"""
Single source of truth (SSoT) for all data models in the Trademark-AI system.

This module defines the core Pydantic models used across the API layer, domain logic,
and tests. These models serve as the canonical schema definitions and should never
be redeclared elsewhere in the codebase.
"""

from typing import Annotated, Literal

from pydantic import BaseModel, Field

# Define EnumStr here for MarkComparison consistency, generated by LLM
EnumStr = Literal["dissimilar", "low", "moderate", "high", "identical"]
# Define OppositionOutcome result literals
OppositionResultEnum = Literal["Opposition likely to succeed", "Opposition may partially succeed", "Opposition likely to fail"]


class Mark(BaseModel):
    """A trademark mark, consisting of text and registration details."""
    wordmark: str = Field(..., description="Literal mark text, case-sensitive")
    is_registered: bool = False
    registration_number: str | None = None


class GoodService(BaseModel):
    """A single goods/services entry with Nice classification."""
    term: str = Field(..., description="The goods/services description")
    nice_class: Annotated[int, Field(ge=1, le=45)] = Field(..., description="Nice classification (1-45)")


class MarkComparison(BaseModel):
    """Comparison results between two trademarks across multiple dimensions."""
    visual: EnumStr = Field(..., description="Visual similarity assessed by LLM")
    aural: EnumStr = Field(..., description="Aural similarity assessed by LLM")
    conceptual: EnumStr = Field(..., description="Conceptual similarity assessed by LLM")
    overall: EnumStr = Field(..., description="Overall mark similarity assessed by LLM")


# New model for Goods/Services Comparison output
class GoodServiceComparison(BaseModel):
    """Detailed comparison between one applicant good/service and one opponent good/service."""
    applicant_good: GoodService = Field(..., description="The applicant's good/service being compared")
    opponent_good: GoodService = Field(..., description="The opponent's good/service being compared")
    overall_similarity: EnumStr = Field(..., description="Overall similarity assessed by LLM (dissimilar, low, moderate, high, identical)")
    are_competitive: bool = Field(..., description="Whether the goods/services are directly competitive")
    are_complementary: bool = Field(..., description="Whether the goods/services are complementary")
    # Add a field for reasoning specific to this pair? Optional for now.
    # reasoning: Optional[str] = Field(None, description="Explanation for this specific G&S comparison")


# New model for the structured opposition outcome
class OppositionOutcome(BaseModel):
    """Structured prediction of the opposition outcome."""
    result: OppositionResultEnum = Field(..., description="The predicted outcome category")
    confidence: Annotated[float, Field(ge=0.0, le=1.0)] = Field(..., description="Confidence score (0.0 to 1.0) for the prediction")
    reasoning: str = Field(..., description="Detailed reasoning supporting the predicted outcome and confidence")


# New model for the LLM-only output of a G&S comparison (no input goods)
class GoodServiceComparisonOutput(BaseModel):
    """Schema for LLM output of a goods/services comparison."""
    overall_similarity: EnumStr = Field(..., description="Overall similarity assessed by LLM (dissimilar, low, moderate, high, identical)")
    are_competitive: bool = Field(..., description="Whether the goods/services are directly competitive")
    are_complementary: bool = Field(..., description="Whether the goods/services are complementary")


# New model for the LLM-only output of the final case prediction (excluding G&S full data)
class CasePredictionOutput(BaseModel):
    """Schema for LLM output of the final case prediction."""
    mark_comparison: MarkComparison = Field(..., description="Detailed mark similarity breakdown generated by LLM (visual, aural, conceptual, overall)")
    likelihood_of_confusion: bool = Field(..., description="Overall likelihood assessment by LLM, considering both marks and G&S")
    opposition_outcome: OppositionOutcome = Field(..., description="Structured prediction of the opposition outcome including reasoning")


# Full prediction including G&S comparisons
class CasePrediction(BaseModel):
    """Complete trademark opposition case prediction including all comparisons."""
    mark_comparison: MarkComparison = Field(..., description="Detailed mark similarity breakdown generated by LLM")
    goods_services_comparisons: list[GoodServiceComparison] = Field(..., description="List of detailed comparisons for each applicant vs. opponent good/service pair")
    likelihood_of_confusion: bool = Field(..., description="Overall likelihood assessment by LLM, considering both marks and G&S")
    opposition_outcome: OppositionOutcome = Field(..., description="Structured prediction of the opposition outcome including reasoning")


# Input structure for the API endpoint
class PredictionRequest(BaseModel):
    """Input model for trademark opposition prediction."""
    applicant: Mark = Field(..., description="The applicant's mark details")
    opponent: Mark = Field(..., description="The opponent's mark details")
    applicant_goods: list[GoodService] = Field(..., max_length=3, description="The applicant's goods/services (max 3 items)")
    opponent_goods: list[GoodService] = Field(..., max_length=3, description="The opponent's goods/services (max 3 items)")


# New model for conceptual similarity score
class ConceptualSimilarityScore(BaseModel):
    """Schema for LLM output of conceptual similarity score."""
    score: Annotated[float, Field(ge=0.0, le=1.0)] = Field(..., description="Conceptual similarity score (0.0 to 1.0)")
