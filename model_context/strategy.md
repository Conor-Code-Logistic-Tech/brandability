# Trademark‚ÄëAI **Greenfield** Blueprint v4 (LLM-Centric)
**What is this app?** A specialised backend API that compares two trademarks (marks + goods/services) and predicts the likelihood and outcome of an opposition under UK/EU law **primarily using an LLM (Gemini 2.5 Pro)** for both similarity assessment and reasoning generation. Results power a lawyer‚Äëfacing UI already hosted elsewhere.
**Target users:** Trademark lawyers and paralegals evaluating conflict risk for clients.

---

## 0 Design Goals
| ID | Goal | Rationale |
|----|------|-----------|
| G‚Äë1 | **Backend‚Äëonly FastAPI on Cloud Run** ‚Äì zero UI/hosting concerns. | Matches existing separately‚Äëhosted front‚Äëend. |
| G‚Äë2 | **Stateless HTTP; no Auth for MVP** ‚Äì public endpoint behind an API Gateway if needed. | Simplifies first release; auth can be toggled later. |
| G‚Äë3 | **LLM-Driven Core**: Gemini 2.5 Pro via Vertex AI handles **all** similarity analysis (visual, aural, conceptual, goods/services) and generates the final prediction + reasoning. | Centralizes logic, leverages LLM's contextual understanding, simplifies dependencies. |
| G‚Äë4 | **Pydantic models as _single source of truth_ (SSoT)** ‚Äì shared by API layer, domain logic, and tests. | Eliminates drift and doubles as JSON schema. |
| G‚Äë5 | **Simplified Dependencies**: Minimize external libraries by relying on the LLM. | Reduces maintenance overhead and potential conflicts. |
| G‚Äë6 | **Smooth DX** ‚Üí Ruff, Pytest (with asyncio support), pre‚Äëcommit. | Consistent, deterministic, and async-aware. |

---

## 1 Repo Layout
```
‚îú‚îÄ‚îÄ api/                        # FastAPI app (presentation + API layer)
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # App entrypoint with prediction endpoint
‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ health.py           # /health endpoint implementation
‚îú‚îÄ‚îÄ trademark_core/             # domain layer (pure python, no FastAPI)
‚îÇ   ‚îú‚îÄ‚îÄ models.py               # Pydantic SSoT ‚Äì see ¬ß2
‚îÇ   ‚îî‚îÄ‚îÄ llm.py                  # LLM integration, prompts, and core prediction logic
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_health.py          # Tests for /health endpoint
‚îÇ   ‚îî‚îÄ‚îÄ test_llm_integration.py # Tests for LLM functionality (critical)
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ model_context/
‚îÇ   ‚îî‚îÄ‚îÄ strategy.md             # This file (v4)
‚îú‚îÄ‚îÄ pyproject.toml              # Project config + tool settings
‚îî‚îÄ‚îÄ README.md
```
*Removed: `trademark_core/similarity.py`, `trademark_core/embeddings.py`, `tests/test_embedding.py`*

---

## 2 Pydantic SSoT (models.py)
All layers import from here ‚Äì never redeclare fields elsewhere.
```python
from typing import Literal, List, Annotated, Dict, Optional
from pydantic import BaseModel, Field

# Define EnumStr here for MarkComparison consistency, generated by LLM
EnumStr = Literal['dissimilar','low','moderate','high','identical']

class Mark(BaseModel):
    wordmark: str = Field(..., description="Literal mark text, case‚Äësensitive")
    is_registered: bool = False
    registration_number: Optional[str] = None

class GoodService(BaseModel):
    term: str
    nice_class: Annotated[int, Field(ge=1, le=45)]

class MarkComparison(BaseModel):
    visual: EnumStr = Field(..., description="Visual similarity assessed by LLM")
    aural: EnumStr = Field(..., description="Aural similarity assessed by LLM")
    conceptual: EnumStr = Field(..., description="Conceptual similarity assessed by LLM")
    overall: EnumStr = Field(..., description="Overall mark similarity assessed by LLM")

class CasePrediction(BaseModel):
    mark_comparison: MarkComparison = Field(..., description="Detailed mark similarity breakdown generated by LLM")
    # goods_services_comparisons removed - handled within reasoning
    likelihood_of_confusion: bool = Field(..., description="Overall likelihood assessment by LLM")
    opposition_outcome: Dict[str, str | float] = Field(..., description="Predicted outcome details generated by LLM (e.g., {result:str, confidence:float, reasoning:str})")

# Input structure for the API endpoint
class PredictionRequest(BaseModel):
    applicant_mark: Mark
    opponent_mark: Mark
    applicant_goods_services: List[GoodService]
    opponent_goods_services: List[GoodService]

```
*Removed `GoodServiceComparison`. `CasePrediction` simplified. Added `PredictionRequest`.*

---

## 3 Core Function (LLM-Centric)
| Function | Input | Output | Implementation |
|---------|-------|--------|----------------|
| `generate_full_prediction` (in `llm.py`) | `PredictionRequest` | `CasePrediction` | Gemini 2.5 Pro via Vertex AI. Constructs a detailed prompt including both marks, both sets of goods/services, and instructions to perform visual, aural, conceptual, G&S similarity analysis, determine likelihood of confusion, predict outcome, and provide comprehensive reasoning. Must handle potential LLM API errors gracefully. |

*Replaced multiple similarity functions with one core LLM function.*

---

## 4 Workflow
1. Front‚Äëend hits `POST /predict` with JSON adhering to `PredictionRequest`.
2. API validates input via Pydantic.
3. Calls `generate_full_prediction` function in `trademark_core.llm`.
4. `generate_full_prediction` constructs a comprehensive prompt for Gemini, sending the applicant/opponent marks and goods/services details.
5. Gemini performs the full analysis (visual, aural, conceptual, G&S similarity), assesses confusion likelihood, predicts the outcome, and generates detailed reasoning.
6. `generate_full_prediction` parses the LLM response, populates the `CasePrediction` model, handling potential errors (e.g., parsing issues, API errors).
7. API returns the `CasePrediction` response or an appropriate error.

*Simplified workflow centered around a single LLM call.*

---

## 5 Key Packages & Services
| Concern | Package | Purpose |
|---------|---------|---------|
| HTTP API | FastAPI + Uvicorn | API framework and server |
| Data Validation | Pydantic | SSoT models and input validation |
| **Core Logic** | **google-genai** / **google-cloud-aiplatform** | **Full similarity assessment, likelihood prediction, and reasoning generation via Gemini LLM** |
| Error Handling | google.api_core.exceptions | Proper LLM error propagation |
| Testing | pytest, pytest-asyncio | Test framework with async support |
| Linting | ruff | Code quality |

*Removed: `python-levenshtein`, `metaphone`, `sentence-transformers`, `transformers`, `nltk`. Updated LLM package purpose.*

---

## 6 Task Breakdown
### Sprint 1 ‚Äì Core LLM Implementation
- [x] Set up project structure in venv with `pip install -r requirements.txt` (update requirements first!)
- [x] Configure linters and testing
- [x] Create FastAPI app with health endpoint
- [x] Implement Pydantic SSoT models (`models.py`)
- [ ] Implement the core `generate_full_prediction` function in `llm.py` including:
    - [ ] Prompt engineering for comprehensive analysis (marks + G&S)
    - [ ] Calling the Gemini API via Vertex AI
    - [ ] Parsing the LLM response into the `CasePrediction` model
    - [ ] Basic error handling for API calls and response parsing
- [ ] Create prediction endpoint in `api/main.py` that uses `generate_full_prediction`
- [ ] Add initial integration tests for the LLM call (`test_llm_integration.py`), mocking the LLM response.

### Sprint 2 ‚Äì Enhancement & Hardening
- [ ] Refine LLM prompt for accuracy and robustness based on test cases.
- [ ] Implement more sophisticated error handling and fallback strategies for LLM integration (e.g., retries, structured error responses).
- [ ] Add comprehensive tests covering various edge cases and LLM failure modes.
- [ ] Add CI/CD pipeline
- [~] Add API documentation _(OpenAPI/Swagger basics configured)_
- [ ] Performance analysis (primarily LLM latency)
- [ ] Security hardening

*Adjusted tasks to focus on LLM implementation and testing.*

---

## 7 Sample Request/Response
```python
# POST /predict
# Uses PredictionRequest structure
{
    "applicant_mark": {
        "wordmark": "CLOUDTECH",
        "is_registered": false
    },
    "opponent_mark": {
        "wordmark": "CLOUDTECHNOLOGY",
        "is_registered": true,
        "registration_number": "UK123456"
    },
    "applicant_goods_services": [
        {
            "term": "Cloud computing services",
            "nice_class": 42
        }
    ],
    "opponent_goods_services": [
        {
            "term": "Data storage solutions",
            "nice_class": 9
        },
        {
            "term": "Platform as a service (PaaS)",
            "nice_class": 42
        }
    ]
}


# Response (CasePrediction structure)
{
    "mark_comparison": {
        "visual": "high",        # Determined by LLM
        "aural": "high",         # Determined by LLM
        "conceptual": "identical", # Determined by LLM
        "overall": "high"        # Determined by LLM
    },
    # "goods_services_comparisons" removed
    "likelihood_of_confusion": true, # Determined by LLM
    "opposition_outcome": {           # Determined by LLM
        "result": "Opposition Likely to Succeed Partially",
        "confidence": 0.75,
        "reasoning": "The marks CLOUDTECH and CLOUDTECHNOLOGY are visually and aurally very similar and conceptually identical, referring to cloud technology. While the opponent's Class 9 goods (Data storage) differ from the applicant's Class 42 services, the opponent also has Class 42 PaaS services which are highly related to the applicant's Cloud computing services. Therefore, confusion is likely regarding the Class 42 offerings..."
    }
}

# Error Response (on LLM failure)
{
    "detail": "Error generating prediction: LLM API Error - Service Unavailable"
}
```
*Updated request/response examples based on revised Pydantic models.*

---

## 8 Acceptance Criteria
| Metric | Target | Status |
|--------|--------|--------|
| Response time P95 | ‚â§ 5s | ‚è±Ô∏è Untested (LLM dependent) |
| Unit test coverage | ‚â• 80% | üîÑ Target (Focus on LLM module) |
| LLM Parsing Success Rate | ‚â• 99% | üîÑ Monitored via error handling |
| Error rate | ‚â§ 0.5% | üîÑ Monitored via error handling |

*Removed mark comparison latency. Adjusted P95 and coverage targets.*

---

## 9 Testing Strategy
| Component | Approach | Status |
|-----------|----------|---------|
| Health Endpoint | Sync + Async tests | ‚úÖ Implemented |
| **LLM Integration** | **Mock LLM responses (success & error cases), test prompt construction, test response parsing, test edge cases (empty inputs, identical inputs, vastly different inputs)** | üîÑ **Highest Priority** |
| API Layer | FastAPI TestClient calling the `/predict` endpoint | üîÑ In Progress |
| Error Handling | Test LLM API error propagation, parsing errors | üîÑ In Progress |

*Removed testing for embeddings/similarity functions. Emphasized comprehensive LLM integration testing.*

---

_End of Blueprint v4 ‚Äì LLM-Centric Implementation_

